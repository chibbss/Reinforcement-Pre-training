{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62097f69-e5a3-4657-a5d7-6b5656afd9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.2.2\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /Users/emmanuelochiba/venvs/myllm310/lib/python3.10/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate\n",
      "---\n",
      "Name: transformers\n",
      "Version: 4.35.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /Users/emmanuelochiba/venvs/myllm310/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: \n",
      "---\n",
      "Name: datasets\n",
      "Version: 2.13.1\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: /Users/emmanuelochiba/venvs/myllm310/lib/python3.10/site-packages\n",
      "Requires: aiohttp, dill, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: \n",
      "---\n",
      "Name: accelerate\n",
      "Version: 0.21.0\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: sylvain@huggingface.co\n",
      "License: Apache\n",
      "Location: /Users/emmanuelochiba/venvs/myllm310/lib/python3.10/site-packages\n",
      "Requires: numpy, packaging, psutil, pyyaml, torch\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show torch transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dcea94-d968-4a35-8294-8a8e3a6dbb5a",
   "metadata": {},
   "source": [
    "# Reinforcement Pre-Training (RPT) ‚Äì Prototype on M1 Mac\n",
    "\n",
    "This notebook is a lightweight prototype of **Reinforcement Pre-Training (RPT)**, inspired by the paper:\n",
    "\n",
    "**\"Reinforcement Pre-Training\" ‚Äî Qingxiu Dong et al., Microsoft Research, 2024**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What Is RPT?\n",
    "\n",
    "Reinforcement Pre-Training reframes **next-token prediction** as a **reinforcement learning problem**. Instead of always optimizing via cross-entropy loss, RPT introduces **verifiable token-level rewards** ‚Äî rewarding the model when it gets the next token right (or does something desirable).\n",
    "\n",
    "- This bridges **language modeling** and **reinforcement learning**\n",
    "- Enables better **reasoning**, **adaptivity**, and **scalability**\n",
    "- Improves alignment with downstream tasks that don‚Äôt rely on cross-entropy\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ What This Notebook Does\n",
    "\n",
    "This is a **minimal, educational prototype** designed to:\n",
    "- Run efficiently on an M1 Mac (no GPUs required)\n",
    "- Use a small `DistilGPT2` model from Hugging Face\n",
    "- Train on a **toy dataset** (custom text examples)\n",
    "- Apply **reward-based token learning**, simulating the RPT logic\n",
    "\n",
    "‚ö†Ô∏è Note: This is not a full reproduction, but a learning-focused approximation to help internalize the core ideas of RPT and build intuition around token-level rewards in transformer pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178440f3-e0f7-4700-9ca7-e3e73317f9a5",
   "metadata": {},
   "source": [
    "### Load & Tokenize Dataset\n",
    "\n",
    "We‚Äôre using a `.jsonl` file made from *The Big Bang Theory* transcripts, reformatted into prompt‚Äìcompletion pairs.\n",
    "\n",
    "Each entry is designed to simulate a conversational prompt and Sheldon-like reply ‚Äî to eventually fine-tune a fun LLM called **SheldonGPT** (more on that soon üòâ). We're keeping things M1-friendly by loading in-memory, disabling dataset caching, and tokenizing each item individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4360e21f-6fa4-4b20-be0a-1c937a4c9a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmanuelochiba/venvs/myllm310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/emmanuelochiba/venvs/myllm310/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/emmanuelochiba/venvs/myllm310/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/emmanuelochiba/venvs/myllm310/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress NumPy 2.0 copy warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT2 doesn't have a pad token, so we use eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13796393-2921-4f09-b973-3049d1765c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONL data (SheldonGPT fine-tuning format)\n",
    "with open(\"sheldon_finetune.jsonl\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513534a-752d-4760-a827-9f281092da78",
   "metadata": {},
   "source": [
    "### Build custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe3e8be-1eb9-4336-9b9a-d2208b29d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.processed_data = []\n",
    "        \n",
    "        for item in data:\n",
    "            text = f\"{item['prompt']}<|endoftext|>{item['completion']}<|endoftext|>\"\n",
    "            encoded = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            self.processed_data.append({\n",
    "                \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": encoded[\"attention_mask\"].squeeze(0)\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1d1e7-7442-4641-9512-1482aa2cb66c",
   "metadata": {},
   "source": [
    "### DataLoader + Safe Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472a2bed-acd9-4cb2-a28c-ec1e6cbf05ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "dataset = CustomTextDataset(data, tokenizer)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=2, \n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd253dd-e7a8-4a86-8698-ad73fd525922",
   "metadata": {},
   "source": [
    "### Load Model & Set Up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5933bd8-ab18-4e22-9563-2077469a4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c98ca-c28d-4766-970f-496793ae29ba",
   "metadata": {},
   "source": [
    "### Reinforcement Pre-Training Loop (Prototype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34f42a4-d65d-4b57-9b00-0b3be56eb59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, RPT Loss: 0.0138\n",
      "Step 20, RPT Loss: 0.0069\n",
      "Step 40, RPT Loss: 0.0069\n",
      "Step 60, RPT Loss: 0.0135\n",
      "Step 80, RPT Loss: 0.0000\n",
      "Step 100, RPT Loss: 0.0000\n",
      "Step 120, RPT Loss: 0.0000\n",
      "Step 140, RPT Loss: 0.0000\n",
      "Step 160, RPT Loss: 0.0450\n",
      "Step 180, RPT Loss: 0.0000\n",
      "Step 200, RPT Loss: 0.0107\n",
      "üèÅ Training completed!\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(dataloader):\n",
    "    if step > 200: break  # Quick loop for M1\n",
    "\n",
    "    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Predictions and reward shaping\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    rewards = (predictions[:, 1:] == labels[:, 1:]).float()\n",
    "\n",
    "    # Shift for next-token alignment\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = labels[:, 1:].contiguous()\n",
    "    shift_rewards = rewards.contiguous()\n",
    "\n",
    "    # Reward-weighted loss\n",
    "    loss = F.cross_entropy(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "        reduction='none'\n",
    "    )\n",
    "    weighted_loss = (loss * shift_rewards.view(-1)).mean()\n",
    "\n",
    "    weighted_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 20 == 0:\n",
    "        print(f\"Step {step}, RPT Loss: {weighted_loss.item():.4f}\")\n",
    "\n",
    "print(\"üèÅ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08940370-b0f1-428f-96da-1e09ac4d8b24",
   "metadata": {},
   "source": [
    "### What We Did\n",
    "\n",
    "‚úÖ Built a mini prototype of **Reinforcement Pre-Training (RPT)** using `distilgpt2` and a custom dataset.\n",
    "\n",
    "‚úÖ Instead of pure next-token loss, we rewarded the model for **token-level correctness**, using the idea of \"verifiable supervision.\"\n",
    "\n",
    "‚úÖ Used a lightweight M1-friendly loop with clean collation and batching (no HuggingFace Datasets cache errors).\n",
    "\n",
    "---\n",
    "\n",
    "üí° The final model isn't production-ready, but this experiment **demonstrates how we might inject RL-style rewards directly into language modeling pretraining**.\n",
    "\n",
    "üß™ Next: Try larger models, smarter rewards (e.g., BLEU, BERTScore), and scale!\n",
    "\n",
    "#llm #rpt #deep_learning #finetuning #openai #huggingface #research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a5cac-7a78-4945-b5a7-208756ef0696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (myllm310)",
   "language": "python",
   "name": "myllm310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
